version: '2'
# networks:
#   mynetwork:
#     external: true
services:
  zookeeper:
    image: wurstmeister/zookeeper
    container_name: ktech_zookeeper
    ports:
     - "2181:2181"
    restart: unless-stopped

  kafka:
    image: wurstmeister/kafka
    container_name: ktech_kafka
    ports:
     - "9094:9094"
    expose:
     - "9093"
    environment:
      KAFKA_ADVERTISED_HOST_NAME: "localhost"
      KAFKA_ADVERTISED_PORT: "9092"
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_CREATE_TOPICS: "reddit-submissions:1:1, reddit-subreddit:1:1"
      KAFKA_LOG_RETENTION_HOURS: 1
      KAFKA_LOG_RETENTION_BYTES: 4073741824
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_RETENTION_CHECK_INTERVAL_MS: 300000
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,OUTSIDE://0.0.0.0:9094
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:9092,OUTSIDE://localhost:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
    volumes:
     - /var/run/docker.sock:/var/run/docker.sock
    restart: unless-stopped

  spark-master:
    image: docker.io/bitnami/spark:3.3
    container_name: ktech_spark 
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    ports:
      - "8080:8080"  # Spark Web UI
      - "7077:7077"
    depends_on:
      - kafka  # Ensure Kafka is up before starting Spark
    volumes:
      - ./:/app/ # Mount your Spark Streaming app
    restart: unless-stopped

  spark-submit:
    image: docker.io/bitnami/spark:3.3
    container_name: ktech_spark_submit
    volumes:
      - ./:/app/
      - ./packages/postgresql-42.5.4.jar:/opt/bitnami/spark/jars/postgresql-42.5.4.jar
    command: ["sh", "-c", "pip install -r /app/requirements.txt && spark-submit --jars /opt/bitnami/spark/jars/postgresql-42.5.4.jar --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.3 --master spark://spark-master:7077 /app/consumer/spark_consumer.py"]
    # ["spark-submit", "--packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.3","--master", "spark://spark-master:7077", "/app/spark_consumer.py"]
    depends_on:
      - spark-master
    restart: unless-stopped
  
  spark-worker:
    image: docker.io/bitnami/spark:3.3
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    ports:
      - "8081:8081"

  postgresql:
    image: postgres:latest
    container_name: ktech_postgresql
    environment:
      POSTGRES_DB: reddit-postgres
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
    ports:
      - "5432:5432"
    volumes:
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init.sql



