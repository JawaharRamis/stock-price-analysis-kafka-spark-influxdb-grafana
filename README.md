---

# Real-Time Stock Data Streaming and Analysis

## Overview

This project is designed to retrieve real-time stock data, perform streaming data analysis using Apache Kafka and Apache Spark, store data in InfluxDB, and visualize the data with Grafana. It aims to provide insights into the stock market and allow users to monitor stock prices effectively.  General information regarding the stock is also retrieved and stored onto a Postrgesql database.

## Table of Contents

- [Prerequisites](#prerequisites)
- [Getting Started](#getting-started)
- [Project Structure](#project-structure)
- [Usage](#usage)
- [Data Flow](#data-flow)
- [Configuration](#configuration)
- [Contributing](#contributing)
- [License](#license)

## Prerequisites

Before you begin, ensure you have met the following requirements:

- Docker and Docker Compose installed
- Python 3.8 or higher
- Kafka, Spark, InfluxDB, and Grafana images available in your Docker environment
- Stock symbols and necessary API keys if applicable

## Getting Started

To get started with this project, follow these steps:

1. Clone the repository to your local machine:

   ```bash
   git clone [https://github.com/JawaharRamis/stock-price-analysis-kafka-spark-influxdb-grafana.git]
   ```

2. Change to the project directory:

   ```bash
   cd real-time-stock-analysis
   ```

3. Run the Docker Compose file to set up the project environment:

   ```bash
   docker-compose up -d
   ```

4. Access the different services:

   - Apache Kafka: http://localhost:9092
   - Apache Spark: http://localhost:8080
   - InfluxDB: http://localhost:8086
   - Grafana: http://localhost:3000

## Project Structure

The project structure is organized as follows:

```
├── producer/              # Python scripts to produce stock data
├── consumer/              # Spark Streaming consumer
├── logs/                  # Log files generated by the application
├── docker-compose.yml     # Docker Compose configuration file
├── .env                   # Environment variables for the project
├── README.md              # Project documentation (this file)
```

## Usage

- **Producer**: The producer scripts (located in the `producer` directory) are responsible for retrieving real-time stock data using `yfinance` library and sending this data to Kafka topics. T

- **Consumer**: The Spark Streaming consumer (located in the `consumer` directory) processes the real-time data and stores it in the timeseries database, InfluxDB.

- **Data Visualization**: Grafana can be used to visualize the stock data stored in InfluxDB. You can create dashboards and panels to display stock prices, trends, and more.

## Data Flow

1. Real-time stock data is retrieved using the producer scripts and sent to Kafka topics.

2. The Spark Streaming consumer reads data from Kafka topics and stores the results in InfluxDB. Before sending real-time stock data to InfluxDB, it is essential to transform the data into a compatible format known as InfluxDB Point. InfluxDB Points are the fundamental unit of data in InfluxDB and consist of the following components:
   - **Measurement**: The measurement name represents the specific dataset or data stream being collected. In our case, the measurement name is typically set to `"stock-price-v1"`.

   - **Tags**: Tags are key-value pairs that provide metadata or labels for the data. For example, a stock symbol like `"AMZN"` can be a tag, allowing you to filter and group data based on the stock       symbol.

   - **Fields**: Fields represent the actual data values associated with the measurement. These are typically numeric values such as stock prices, volumes, or other metrics.

   - **Timestamp**: Each InfluxDB Point has a timestamp, indicating when the data was recorded.

3. Grafana is used to visualize and monitor the stock data stored in InfluxDB.

## Configuration

- Configuration for various services, such as Kafka, Spark, InfluxDB, and Grafana, can be found in the `docker-compose.yml` file.

- Environment variables can be configured in the `.env` file.

## Contributing

Contributions to this project are welcome! To contribute:

1. Fork the repository.
2. Create a new branch for your feature or bug fix.
3. Make your changes and submit a pull request.


Sure, here's an updated section in your `readme.md` that explains how data is converted into a compatible format (e.g., InfluxDB Point) before being sent to InfluxDB:

---

## Data Transformation and Storage

### Data Format for InfluxDB

Before sending real-time stock data to InfluxDB, it is essential to transform the data into a compatible format known as InfluxDB Point. InfluxDB Points are the fundamental unit of data in InfluxDB and consist of the following components:



### Data Transformation Process

The data transformation process involves converting raw real-time stock data into InfluxDB Points with the appropriate measurement, tags, fields, and timestamps.

1. **Measurement**: The measurement name is typically set to `"stock-price-v1"` to represent the stock price data stream.

2. **Tags**: Tags are used to label data points with additional information. In the context of stock data, tags often include the stock symbol, allowing you to filter and query data for specific stocks.

3. **Fields**: Fields contain the actual data values, such as stock prices, volume, and other relevant metrics.

4. **Timestamp**: Each data point is associated with a timestamp indicating when the data was recorded. This timestamp is crucial for time-series data analysis and visualization.

### Sending Data to InfluxDB

Once the real-time stock data is transformed into InfluxDB Points, it can be sent to InfluxDB for storage and further analysis. The Spark Streaming consumer in this project is responsible for performing this transformation and writing the data to InfluxDB.

By using InfluxDB Points, the project ensures that the data is in the correct format for efficient storage and querying within InfluxDB. This structured approach makes it easier to manage and analyze large volumes of time-series data efficiently.

---

Feel free to adjust the content and level of detail as needed to match your project's specific data transformation process and requirements.
